# This docker-compose is for developer convenience, not for running in production.

services:

  postgres:  # for hive
    image: postgres:16.3
    ports:
      - 5432:5432
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword
      POSTGRES_DB: hive

  minio:
    image: minio/minio:RELEASE.2025-02-07T23-21-09Z
    environment:
      MINIO_ROOT_USER: miniouser
      MINIO_ROOT_PASSWORD: miniopassword
    ports:
      - 9000:9000
      - 9001:9001
    command: 'server /data --console-address ":9001"'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 1s
      retries: 5

  minio-create-bucket:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MINIO_URL: http://minio:9000
      MINIO_USER: miniouser
      MINIO_PWD: miniopassword
      MINIO_IMPORTER_USER: importers
      MINIO_IMPORTER_PWD: impassword
      MINIO_IMPORTER_RAW_BUCKET: cts-output
      MINIO_IMPORTER_SQL_BUCKET: cdm-lake
    entrypoint: /s3_policies/minio_create_bucket_entrypoint.sh
    volumes:
      - ./s3_policies:/s3_policies

  spark-master:
    image: ghcr.io/kbase/cdm-spark-standalone:pr-36
    platform: linux/amd64
    ports:
      - 8090:8090
    environment:
      SPARK_MODE: master
      SPARK_MASTER_WEBUI_PORT: 8090
      MAX_EXECUTORS: 4
      EXECUTOR_CORES: 2
      MAX_CORES_PER_APPLICATION: 10
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword
      POSTGRES_DB: hive
      POSTGRES_URL: postgres:5432
      DATANUCLEUS_AUTO_CREATE_TABLES: true
      DELTALAKE_WAREHOUSE_DIR: s3a://cdm-lake/warehouse

  spark-worker-1:
    image: ghcr.io/kbase/cdm-spark-standalone:pr-36
    platform: linux/amd64
    depends_on:
      - spark-master
    ports:
      - 8081:8081
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_WEBUI_PORT: 8081
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword
      POSTGRES_DB: hive
      POSTGRES_URL: postgres:5432
      DATANUCLEUS_AUTO_CREATE_TABLES: true
      DELTALAKE_WAREHOUSE_DIR: s3a://cdm-lake/warehouse

  spark-worker-2:
    image: ghcr.io/kbase/cdm-spark-standalone:pr-36
    platform: linux/amd64
    depends_on:
      - spark-master
    ports:
      - 8082:8082
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_WEBUI_PORT: 8082
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword
      POSTGRES_DB: hive
      POSTGRES_URL: postgres:5432
      DATANUCLEUS_AUTO_CREATE_TABLES: true
      DELTALAKE_WAREHOUSE_DIR: s3a://cdm-lake/warehouse

  # A test image for testing purposes. The importers repo is built into the event processor,
  # and so does not have an independent image.
  test-container:  # do not use underscore or spark will barf
    build:
      context: .
      dockerfile: Dockerfile
    platform: linux/amd64
    depends_on:
      postgres:
        condition: service_started
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
      minio-create-bucket:
        condition: service_completed_successfully
    environment:
      # test variables used by the test helper code
      IMP_MINIO_URL: http://minio:9000
      IMP_MINIO_ACCESS_KEY: importers
      IMP_MINIO_SECRET_KEY: impassword
      IMP_SPARK_MASTER_URL: spark://spark-master:7077
      IMP_SPARK_DRIVER_HOST: test-container
      IMP_MINIO_IMPORTER_RAW_BUCKET: cts-output

      # variables used by the base image
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword
      POSTGRES_DB: hive
      POSTGRES_URL: postgres:5432
      DELTALAKE_WAREHOUSE_DIR: s3a://cdm-lake/warehouse
      # Importers should not implicitly create tables, but hive won't start without this
      DATANUCLEUS_AUTO_CREATE_TABLES: true

      # By default the mounted code directory is not writeable by spark_user. As such,
      # when running tests with coverage:
      # 1) Create a coverage dir in the repo root and make it world writeable.
      # 2) Instruct pytest to write coverage files there,
      #    e.g. --cov-report xml:./coverage/coverage.xml 
      COVERAGE_FILE: /imp_test/coverage/.coverage
    volumes:
      - ./:/imp_test
